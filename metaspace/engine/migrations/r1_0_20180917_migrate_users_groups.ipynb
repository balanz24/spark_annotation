{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "engine_path = '/opt/dev/metaspace/metaspace/engine'\n",
    "if engine_path not in set(sys.path):\n",
    "    sys.path.append(engine_path)\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting, log in to SQL as the `postgres` user and run this in the `sm` database:\n",
    "```sql\n",
    "CREATE SCHEMA graphql;\n",
    "CREATE EXTENSION \"uuid-ossp\";\n",
    "ALTER SCHEMA graphql OWNER TO sm;\n",
    "```\n",
    "Then run the 1.0 version of GraphQL so that TypeORM builds the new database structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from elasticsearch.client import IndicesClient, IngestClient\n",
    "from sm.engine.db import DB\n",
    "from sm.engine.es_export import ESExporter, init_es_conn\n",
    "from sm.engine.util import SMConfig\n",
    "from sm.engine.dataset import Dataset\n",
    "SMConfig.set_path('../conf/config.json')\n",
    "sm_config = SMConfig.get_conf()\n",
    "db = DB(sm_config['db'], autocommit=True)\n",
    "es = ESExporter(db)\n",
    "es_conn = init_es_conn(sm_config['elasticsearch'])\n",
    "es_index = sm_config['elasticsearch']['index']\n",
    "ingest = IngestClient(es_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitter_data_file = \"r1_0_20180917_submitter_data.csv\"\n",
    "group_membership_file = \"r1_0_20180917_group_membership.csv\"\n",
    "\n",
    "group_keys = ['institution', 'email', 'name', 'pi_email', 'pi_name']\n",
    "dirty_group_keys = ['current_' + key for key in group_keys]\n",
    "clean_group_keys = ['new_' + key for key in group_keys]\n",
    "\n",
    "SubmitterInfo = namedtuple('SubmitterInfo', group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all institutions, submitters and PIs (now referred to as \"submitter data\")\n",
    "def get_src_datasets():\n",
    "    def normalize_name(person):\n",
    "        if person.get('Name'):\n",
    "            return person['Name']\n",
    "        return (person.get('First_Name', '').strip() + ' ' + person.get('Surname', '').strip()).strip()\n",
    "\n",
    "    src_datasets = []\n",
    "    for id, is_public, metadata in db.select(\"select id, is_public, metadata from dataset\", None):\n",
    "        submitted_by = metadata.get('Submitted_By')\n",
    "        if submitted_by and submitted_by.get('Submitter'):\n",
    "            submitter = submitted_by['Submitter']\n",
    "            pi = submitted_by.get('Principal_Investigator', {})\n",
    "            src_datasets.append({\n",
    "                \"id\": id,\n",
    "                \"is_public\": is_public,\n",
    "                \"institution\": submitted_by.get('Institution', ''),\n",
    "                \"email\": submitter.get('Email', '').lower(),\n",
    "                \"name\": normalize_name(submitter),\n",
    "                \"pi_email\": pi.get('Email', '').lower(),\n",
    "                \"pi_name\": normalize_name(pi),\n",
    "            })\n",
    "    return src_datasets\n",
    "\n",
    "src_datasets = get_src_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump submitter data to file for manual cleaning\n",
    "def dump_submitters_for_manual_cleaning(filename, src_datasets):\n",
    "    groups = (pd.DataFrame(src_datasets)\n",
    "        .sort_values(group_keys)\n",
    "        .groupby(group_keys))\n",
    "\n",
    "    rows = []\n",
    "    for groupkey, group in groups:\n",
    "        rows.append(dict(\n",
    "            [('datasets', group.size)] +\n",
    "            [('earliest', group['id'].min())] +\n",
    "            [('latest', group['id'].max())] +\n",
    "            list(zip(dirty_group_keys, groupkey)) +\n",
    "            list(zip(clean_group_keys, groupkey))\n",
    "        ))\n",
    "        \n",
    "    df = pd.DataFrame(rows)\n",
    "    # Reorder columns & export\n",
    "    df = df[['datasets', 'earliest', 'latest', *dirty_group_keys, *clean_group_keys]].to_csv(filename, index=False)\n",
    "\n",
    "if not os.path.isfile(submitter_data_file):\n",
    "    dump_submitters_for_manual_cleaning(submitter_data_file, src_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now manually edit the CSV, leaving the \"current_\" columns as-is and cleaning the values in the \"new_\" columns***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned CSV, unpack it and validate that it is consistent\n",
    "    \n",
    "def read_cleaned_submitters(submitter_file):\n",
    "    combined_cleaned_data = pd.read_csv(submitter_data_file).fillna('')\n",
    "    dirty_data = combined_cleaned_data.loc[:, dirty_group_keys].rename(columns=dict(zip(dirty_group_keys, group_keys)))\n",
    "    clean_data = combined_cleaned_data.loc[:, clean_group_keys].rename(columns=dict(zip(clean_group_keys, group_keys)))\n",
    "    \n",
    "    groups_to_add = set(institution for (institution,) in clean_data[['institution']].values if institution)\n",
    "    submitters_to_add = set((email, name) for email, name in clean_data[['email','name']].values)\n",
    "    pis_to_add = set((email, name) for email, name in clean_data[['pi_email','pi_name']].values if email and name)\n",
    "    users_to_add = submitters_to_add.union(pis_to_add)\n",
    "    \n",
    "    dirty_to_clean = dict(zip(\n",
    "        (SubmitterInfo(*args) for args in dirty_data.values), \n",
    "        (SubmitterInfo(*args) for args in clean_data.values)))\n",
    "    \n",
    "    return dirty_data, clean_data, dirty_to_clean, groups_to_add, users_to_add\n",
    "\n",
    "def validate_cleaned_submitters(src_datasets, clean_data, dirty_to_clean, users_to_add):\n",
    "    for email, df in pd.DataFrame(list(users_to_add), columns=[\"email\",\"name\"]).groupby([\"email\"], as_index=False):\n",
    "        if len(df['name'].values) > 1:\n",
    "            print(f'Submitter/PI email mapped to multiple names: {email} -> {\", \".join(df[\"name\"].values)}')\n",
    "\n",
    "    for idx, series in clean_data.iterrows():\n",
    "        inst, email, name = series.loc[['institution', 'pi_email', 'pi_name']]\n",
    "        if not inst and not (email and name):\n",
    "            print(f'Row {idx+1} has no institution or PI')\n",
    "\n",
    "    for ds in src_datasets:\n",
    "        key = SubmitterInfo(*(ds[key] for key in group_keys))\n",
    "        if not dirty_to_clean.get(key):\n",
    "            print(f'Missing cleaned data for {key}')\n",
    "            \n",
    "dirty_data, clean_data, dirty_to_clean, groups_to_add, users_to_add = read_cleaned_submitters(submitter_data_file)\n",
    "validate_cleaned_submitters(src_datasets, clean_data, dirty_to_clean, users_to_add)\n",
    "# Fix validation errors & rerun if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new groups & users\n",
    "for group in groups_to_add:\n",
    "    if not db.select_one(\"SELECT 1 FROM graphql.group WHERE name = %s\", [group]):\n",
    "        db.alter(\"INSERT INTO graphql.group (name, short_name) VALUES (%s, %s)\", [group, group])\n",
    "    \n",
    "for email, name in users_to_add:\n",
    "    if not db.select_one(\"SELECT 1 FROM graphql.user WHERE LOWER(email) = LOWER(%s)\", [email]):\n",
    "        db.alter(\"\"\"\n",
    "            WITH cred AS (INSERT INTO graphql.credentials (\"email_verified\") VALUES (FALSE) RETURNING id)\n",
    "            INSERT INTO graphql.user (not_verified_email, name, role, credentials_id)\n",
    "            SELECT %s, %s, 'user', cred.id\n",
    "            FROM cred;\n",
    "        \"\"\", [email, name])\n",
    "    else:\n",
    "        db.alter('UPDATE graphql.user SET name = %s WHERE email = %s', [name, email])\n",
    "\n",
    "new_group_name_to_id = dict(db.select(\"SELECT name, id FROM graphql.group\"))\n",
    "new_user_email_to_id = dict(db.select(\"SELECT COALESCE(email, not_verified_email), id FROM graphql.user\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_additional_info(metadata):\n",
    "    section = metadata.get('Additional_Information', {})\n",
    "\n",
    "    if section.get('Supplementary') == None:\n",
    "        vals = []\n",
    "\n",
    "        if section.get('Additional_Information_Freetext', '').strip():\n",
    "            vals.append(section['Additional_Information_Freetext'])\n",
    "        if section.get('Sample_Description_Freetext', '').strip():\n",
    "            vals.append('Sample Description: ' + section['Sample_Description_Freetext'])\n",
    "        if section.get('Sample_Preparation_Freetext', '').strip():\n",
    "            vals.append('Sample Preparation: ' + section['Sample_Preparation_Freetext'])\n",
    "        if section.get('Expected_Molecules_Freetext', '').strip():\n",
    "            vals.append('Expected Molecules: ' + section['Expected_Molecules_Freetext'])\n",
    "        if section.get('Publication_DOI', '').strip():\n",
    "            vals.append('Publication DOI: ' + section['Publication_DOI'])\n",
    "        \n",
    "        metadata['Additional_Information'] = {'Supplementary': '\\n'.join(vals)}\n",
    "\n",
    "# Update datasets\n",
    "def update_dataset(ds, dirty_to_clean, new_group_name_to_id, new_user_email_to_id):\n",
    "    ds_id = ds['id']\n",
    "    cleaned = dirty_to_clean[SubmitterInfo(*(ds[key] for key in group_keys))]\n",
    "    user_id = new_user_email_to_id[cleaned.email]\n",
    "    group_id = cleaned.institution and new_group_name_to_id[cleaned.institution] or None\n",
    "    metadata, = db.select_one('SELECT metadata FROM dataset WHERE id = %s', [ds_id])\n",
    "    submitter_name_parts = cleaned.name.split(' ', 1)\n",
    "    pi_name_parts = (cleaned.pi_name or '').split(' ', 1)\n",
    "    \n",
    "    if not db.select_one('SELECT 1 FROM graphql.dataset WHERE id = %s', [ds_id]):\n",
    "        db.alter(\"\"\"\n",
    "                INSERT INTO graphql.dataset (id, user_id, group_id, group_approved)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                 \"\"\", [ds_id, user_id, group_id, True])\n",
    "    else:\n",
    "        db.alter(\"UPDATE graphql.dataset SET user_id = %s, group_id = %s WHERE id = %s\", [user_id, group_id, ds_id])\n",
    "    \n",
    "    ds.update(cleaned._asdict())\n",
    "    \n",
    "    if 'Submitted_By' in metadata:\n",
    "        del metadata['Submitted_By']\n",
    "    join_additional_info(metadata)\n",
    "    \n",
    "    db.alter('UPDATE dataset SET metadata = %s WHERE id = %s', [json.dumps(metadata), ds_id])\n",
    "    \n",
    "for i, ds in enumerate(src_datasets):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    update_dataset(ds, dirty_to_clean, new_group_name_to_id, new_user_email_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump calculated group memberships for manual checking\n",
    "def dump_group_membership_for_manual_cleaning(filename, clean_data):\n",
    "    src_datasets_df = pd.DataFrame(src_datasets)\n",
    "    institutions_and_emails = np.concatenate([\n",
    "        clean_data[['institution','email','name']].values,\n",
    "        clean_data[['institution','pi_email','pi_name']].values])\n",
    "\n",
    "    columns = ['group', 'email', 'name', 'datasets submitted', 'datasets as PI', 'datasets as PI to someone else', 'role']\n",
    "    users_in_groups = []\n",
    "    for inst, email, name in pd.unique([(inst, email, name) for inst, email, name in institutions_and_emails]):\n",
    "        if inst and email:\n",
    "            df_inst = src_datasets_df['institution'] == inst\n",
    "            df_subm = src_datasets_df['email'] == email\n",
    "            df_not_subm = src_datasets_df['email'] != email\n",
    "            df_pi = src_datasets_df['pi_email'] == email\n",
    "            datasets_as_submitter = src_datasets_df[df_inst & df_subm].shape[0]\n",
    "            datasets_as_pi = src_datasets_df[df_inst & df_pi].shape[0]\n",
    "            datasets_as_pi_to_someone_else = src_datasets_df[df_inst & df_pi & df_not_subm].shape[0]\n",
    "            role = 'PRINCIPAL_INVESTIGATOR' if datasets_as_pi > 0 else 'MEMBER'\n",
    "            users_in_groups.append([inst, email, name, datasets_as_submitter, datasets_as_pi, datasets_as_pi_to_someone_else, role])\n",
    "        \n",
    "    df = pd.DataFrame(users_in_groups, columns=columns).sort_values(columns)\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    \n",
    "if not os.path.isfile(group_membership_file):\n",
    "    dump_group_membership_for_manual_cleaning(group_membership_file, clean_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and validate group memberships\n",
    "    \n",
    "def read_cleaned_group_membership(filename):\n",
    "    data = pd.read_csv(filename).fillna('')\n",
    "    data = data[['group', 'email', 'role']]\n",
    "    \n",
    "    #validate\n",
    "    valid_data = []\n",
    "    for idx, (group, email, role) in enumerate(data.values):\n",
    "        if not new_group_name_to_id.get(group):\n",
    "            print(f\"Unrecognized group {group} on line {idx}\")\n",
    "        elif not new_user_email_to_id.get(email):\n",
    "            print(f\"Unrecognized user {email} on line {idx}\")\n",
    "        elif not role in ['PRINCIPAL_INVESTIGATOR','MEMBER', '']:\n",
    "            print(f\"Unrecognized role {role} on line {idx}\")\n",
    "        elif role != '':\n",
    "            valid_data.append((group, email, role))\n",
    "            \n",
    "    return valid_data\n",
    "            \n",
    "group_membership = read_cleaned_group_membership(group_membership_file)\n",
    "\n",
    "# Fix data and rerun this cell if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, email, role in group_membership:\n",
    "    group_id = new_group_name_to_id[group]\n",
    "    user_id = new_user_email_to_id[email]\n",
    "    if not db.select_one('SELECT 1 FROM graphql.user_group WHERE group_id = %s AND user_id = %s', [group_id, user_id]):\n",
    "        db.alter(\"INSERT INTO graphql.user_group (group_id, user_id, role) VALUES (%s, %s, %s)\", [group_id, user_id, role])\n",
    "    else:\n",
    "        db.alter(\"UPDATE graphql.user_group SET role = %s WHERE group_id = %s AND user_id = %s\", [role, group_id, user_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now rebuild the ES index***\n",
    "```bash\n",
    "source activate sm\n",
    "python -m scripts.create_es_index --drop\n",
    "python -m scripts.update_es_index --ds-name %\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (sm)",
   "language": "python",
   "name": "sm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
